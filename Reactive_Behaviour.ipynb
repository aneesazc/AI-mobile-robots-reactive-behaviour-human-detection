{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reactive Behaviour Implementation\n",
    "\n",
    "This program is used to demontrate the following four Braitenberg behaviours: aggressive, bear, and love as well as curious behaviours.\n",
    "The first step is to read the labels.txt file. We also needed to download the pretrained model ('alexnet-owt-4df8aa71.pth') and upload it to the robot. The model can be downloaded from the following link https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# code is from source[1]\n",
    "#open the txt file and read object categories.\n",
    "f = open(\"labels.txt\", \"r\") \n",
    "labels = []\n",
    "for x in f: # read the txt file line by line\n",
    "    labels.append(x.split(':')[1]) #add it to the list\n",
    "print(len(labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to capture image data from the RGBD sensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use traitlets and widgets to display the image in Jupyter Notebook\n",
    "import traitlets\n",
    "from traitlets.config.configurable import SingletonConfigurable\n",
    "\n",
    "#use opencv to covert the depth image to RGB image for displaying purpose\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#using realsense to capture the color and depth image\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "#multi-threading is used to capture the image in real time performance\n",
    "import threading\n",
    "\n",
    "predict_id = 0\n",
    "class Camera(SingletonConfigurable):\n",
    "    \n",
    "    #this changing of this value will be captured by traitlets\n",
    "    color_value = traitlets.Any()\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Camera, self).__init__()\n",
    "        \n",
    "        #configure the color and depth sensor\n",
    "        self.pipeline = rs.pipeline()\n",
    "        self.configuration = rs.config()  \n",
    "        \n",
    "        #set resolution for the color camera\n",
    "        self.color_width = 640\n",
    "        self.color_height = 480\n",
    "        self.color_fps = 30\n",
    "        self.configuration.enable_stream(rs.stream.color, self.color_width, self.color_height, rs.format.bgr8, self.color_fps)\n",
    "\n",
    "        #set resolution for the depth camera\n",
    "        self.depth_width = 640\n",
    "        self.depth_height = 480\n",
    "        self.depth_fps = 30\n",
    "        self.configuration.enable_stream(rs.stream.depth, self.depth_width, self.depth_height, rs.format.z16, self.depth_fps)\n",
    "\n",
    "        #flag to control the thread\n",
    "        self.thread_runnning_flag = False\n",
    "        \n",
    "        #start the RGBD sensor\n",
    "        self.pipeline.start(self.configuration)\n",
    "        self.pipeline_started = True\n",
    "        frames = self.pipeline.wait_for_frames()\n",
    "\n",
    "        #start capture the first color image\n",
    "        color_frame = frames.get_color_frame()   \n",
    "        image = np.asanyarray(color_frame.get_data())\n",
    "        self.color_value = image\n",
    "\n",
    "        #start capture the first depth image\n",
    "        depth_frame = frames.get_depth_frame()           \n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "        self.depth_value = depth_colormap   \n",
    "\n",
    "\n",
    "    def _capture_frames(self):\n",
    "        while(self.thread_runnning_flag==True): #continue until the thread_runnning_flag is set to be False\n",
    "            frames = self.pipeline.wait_for_frames() #receive data from RGBD sensor\n",
    "            \n",
    "            color_frame = frames.get_color_frame() #get the color image\n",
    "            image = np.asanyarray(color_frame.get_data()) #convert color image to numpy array\n",
    "            self.color_value = image #assign the numpy array image to the color_value variable \n",
    "\n",
    "            depth_frame = frames.get_depth_frame() #get the depth image           \n",
    "            depth_image = np.asanyarray(depth_frame.get_data()) #convert depth data to numpy array\n",
    "                \n",
    "            #we only consider the central area of the vision sensor\n",
    "            depth_image[:100,:]=0\n",
    "            depth_image[350:,:]=0\n",
    "            depth_image[:,:100]=0\n",
    "            depth_image[:,550:]=0\n",
    "            \n",
    "            #For object avoidance, we don't consider the distance that are lower than 100mm or bigger than 1000mm\n",
    "            depth_image[depth_image<100]=0\n",
    "\n",
    "            \n",
    "            #If all of the values in the depth image is 0, the depth[depth!=0] command will fail\n",
    "            #we set a specific value here to prevent this failure\n",
    "            depth_image[0,0]=2000\n",
    "            \n",
    "            depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "            if (depth_image[depth_image!=0].min()<650):\n",
    "\n",
    "                self.warning_flag=1\n",
    "            else:\n",
    "                self.warning_flag=0\n",
    "            self.depth_value = depth_colormap               \n",
    "    \n",
    "    def start(self): #start the data capture thread\n",
    "        if self.thread_runnning_flag == False: #only process if no thread is running yet\n",
    "            self.thread_runnning_flag=True #flag to control the operation of the _capture_frames function\n",
    "            self.thread = threading.Thread(target=self._capture_frames) #link thread with the function\n",
    "            self.thread.start() #start the thread\n",
    "\n",
    "    def stop(self): #stop the data capture thread\n",
    "        if self.thread_runnning_flag == True:\n",
    "            self.thread_runnning_flag = False #exit the while loop in the _capture_frames\n",
    "            self.thread.join() #wait the exiting of the thread       \n",
    "\n",
    "def bgr8_to_jpeg(value):#convert numpy array to jpeg coded data for displaying \n",
    "    return bytes(cv2.imencode('.jpg',value)[1])\n",
    "\n",
    "#create a camera object\n",
    "camera = Camera.instance()\n",
    "camera.start() # start capturing the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to perform classification and display the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06beaedff67b4d198f4f371c979fd969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'', format='jpeg', width='45%'), Image(value=b'', format='jpeg', width='45%')), laâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#The pytorch platform is used in this tutorial\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# the following AlexNet model is defined by torchvision\n",
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = AlexNet()\n",
    "model.load_state_dict(torch.load('alexnet-owt-4df8aa71.pth'))\n",
    "# 'alexnet-owt-4df8aa71.pth' is the pretrained model, it can be downloaded from the following link\n",
    "# https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\n",
    "# this model should be placed under the same folder of this file\n",
    "\n",
    "#We use GPU for classification \n",
    "device = torch.device('cuda')\n",
    "model = model.to(device)\n",
    "\n",
    "import time\n",
    "from RobotClass import Robot\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "import sys\n",
    "\n",
    "#create widgets for the displaying of the image\n",
    "display_color = widgets.Image(format='jpeg', width='45%') #determine the width of the color image\n",
    "display_depth = widgets.Image(format='jpeg', width='45%')  #determine the width of the depth image\n",
    "layout=widgets.Layout(width='100%')\n",
    "\n",
    "sidebyside = widgets.HBox([display_color, display_depth],layout=layout) #horizontal \n",
    "display(sidebyside) #display the widget\n",
    "\n",
    "#callback function, invoked when traitlets detects the changing of the color image\n",
    "def process(change):\n",
    "    \n",
    "    image = change['new'] #retrieve data from the input dict\n",
    "    display_color.value = bgr8_to_jpeg(cv2.resize(image,(320,240)))\n",
    "    display_depth.value = bgr8_to_jpeg(cv2.resize(camera.depth_value,(320,240)))\n",
    "\n",
    "#processing({'new': camera.color_value})\n",
    "#the camera.observe function will monitor the color_value variable. If this value changes, the processing function will be excuted.\n",
    "camera.observe(process, names='color_value')\n",
    "\n",
    "\n",
    "#the following code will classify the image and measure the time \n",
    "while (1):\n",
    "    t1 = cv2.getTickCount() \n",
    "    imgsized= cv2.resize(camera.color_value,(224,224)) #resize the image\n",
    "    x = cv2.cvtColor(imgsized, cv2.COLOR_BGR2RGB) #convert to RGB as required by the model\n",
    "    x = x.transpose((2, 0, 1)) #swith the image channels\n",
    "    x = torch.from_numpy(x).float() #convert to type float\n",
    "    mean = 255.0 * np.array([0.485, 0.456, 0.406]) #mean value\n",
    "    stdev = 255.0 * np.array([0.229, 0.224, 0.225]) # for the nomalization of the input image\n",
    "    normalize = torchvision.transforms.Normalize(mean, stdev)\n",
    "    x = normalize(x)\n",
    "    x = x.to(device)# send the data to GPU device\n",
    "    x = x[None, ...] # increase the image dimension, the model takes a batch of images and the batch size is 1\n",
    "    output = model(x) #classfy the images\n",
    "    predict_id = output.max(1, keepdim=True)[1].item() #get the label\n",
    "    #print('id ', predict_id,\" prediction time \", (cv2.getTickCount()-t1)/cv2.getTickFrequency())\n",
    "    \n",
    "    #end of source[1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #initialize the Robot class\n",
    "    robot = Robot()\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "# Start of Function definition for reactive behaviours:\n",
    "\n",
    "    def aggression():\n",
    "        '''\n",
    "        \n",
    "        This function is used to show the emotion of aggression\n",
    "        on detecting a running shoe.\n",
    "        \n",
    "        '''\n",
    "        robot.forward(1)  # goes forward full speed\n",
    "        time.sleep(0.5)\n",
    "        robot.backward(0.2)  # goes a bit backward to avoid crashing with the object\n",
    "        time.sleep(0.3)\n",
    "        robot.stop()\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    def fear():\n",
    "        '''\n",
    "        \n",
    "        This function is used to show the emotion of fear\n",
    "        on detecting a water bottle.\n",
    "        \n",
    "        Movement : At first, it moves forward a little, after detecting\n",
    "        the 'water bottle',it goes a bit backward at a slow speed\n",
    "        and then it goes backward at full speed. After that, it turns \n",
    "        right to change direction and escape from the intended object by \n",
    "        moving a bit forward.\n",
    "        \n",
    "        '''\n",
    "        robot.forward(0.5)\n",
    "        time.sleep(0.2)\n",
    "        robot.backward(0.7)\n",
    "        time.sleep(0.3)\n",
    "        robot.backward(1)\n",
    "        time.sleep(0.5)\n",
    "        robot.right(0.6)\n",
    "        time.sleep(0.6)\n",
    "        robot.right(0.6)\n",
    "        time.sleep(0.6)\n",
    "        robot.forward(0.9)\n",
    "        time.sleep(1)\n",
    "        robot.stop()\n",
    "            \n",
    "         \n",
    "        \n",
    "    def love():\n",
    "        '''\n",
    "        \n",
    "        This function is used to show the emotion of love\n",
    "        on detecting a 'cellphone'.\n",
    "        \n",
    "        Movement : At first, it turns left, after detecting\n",
    "        the 'cellphone', it goes forward and turns right\n",
    "        8 times to form a circle which shows the robot's interest\n",
    "        on the object.\n",
    "        \n",
    "        '''\n",
    "\n",
    "       \n",
    "        robot.left(0.6) \n",
    "        time.sleep(0.5)\n",
    "        for i in range(6):\n",
    "            robot.forward(0.6)\n",
    "            time.sleep(0.6)\n",
    "            robot.right(0.6)\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        robot.stop()\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    def look(speed,t):\n",
    "        '''\n",
    "        \n",
    "        This function is used to make robot looking around\n",
    "        using robot.left and robot.right on detecting an umbrella.\n",
    "        \n",
    "        '''\n",
    "        robot.right(speed)\n",
    "        time.sleep(t)\n",
    "        robot.left(speed)\n",
    "        time.sleep(t)\n",
    "        robot.right(speed)\n",
    "       \n",
    "    \n",
    "    def curious():\n",
    "        '''\n",
    "        This function is used to show the behaviour of curiosity on detecting an umbrella.\n",
    "        \n",
    "        Movement : when robot detects an umbrella, at first, by using look function the robot \n",
    "        starts to look around itself and then go forward to get close to the object.\n",
    "        \n",
    "        '''\n",
    "        look(0.5,0.3)\n",
    "        robot.forward(0.4)\n",
    "        time.sleep(0.3)\n",
    "        look(0.5,0.3)\n",
    "        robot.forward(0.4)\n",
    "        time.sleep(0.3)\n",
    "        robot.stop()\n",
    "            \n",
    "            \n",
    "            \n",
    "# End of function definition for reactive behaviour     \n",
    "            \n",
    "            \n",
    "  \n",
    "\n",
    "            \n",
    "    if(camera.warning_flag):        \n",
    "        if predict_id == 770: #running shoe\n",
    "                aggression()\n",
    "        elif predict_id == 487: #cellphone\n",
    "                fear()\n",
    "        elif predict_id == 898: #water bottle\n",
    "                love()\n",
    "        elif predict_id == 879: #umbrella\n",
    "                curious()\n",
    "    else:\n",
    "            robot.stop()\n",
    "        \n",
    "\n",
    "\n",
    "#source[1] https://learn.lboro.ac.uk/mod/resource/view.php?id=1042759"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "source[1] https://learn.lboro.ac.uk/mod/resource/view.php?id=1042759"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
