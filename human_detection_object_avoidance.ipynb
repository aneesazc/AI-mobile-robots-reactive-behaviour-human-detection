{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Avoidance and Human Following\n",
    "\n",
    "This program will detect and follow a human whilst maintaing a safe distance. The ssd mobilenet v2 nerual network structure is used for human detection and object avoidance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 load the pre-trained object detection model\n",
    "\n",
    "**Before running the program, we need to download the pre-trained detection model and put it in the current folder.** The model can be found in the following link:https://drive.google.com/file/d/1KjlDMRD8uhgQmQK-nC2CZGHFTbq4qQQH/view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "# Source [1]\n",
    "import tensorrt as trt\n",
    "from tensorrt_model import TRTModel\n",
    "from ssd_tensorrt import load_plugins, parse_boxes,TRT_INPUT_NAME, TRT_OUTPUT_NAME\n",
    "import ctypes\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import ctypes\n",
    "    \n",
    "mean = 255.0 * np.array([0.5, 0.5, 0.5])\n",
    "stdev = 255.0 * np.array([0.5, 0.5, 0.5])\n",
    "\n",
    "## Define all the essential functions \n",
    "def bgr8_to_ssd_input(camera_value):\n",
    "    ''''''\n",
    "    x = camera_value\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = x.transpose((2, 0, 1)).astype(np.float32)\n",
    "    x -= mean[:, None, None]\n",
    "    x /= stdev[:, None, None]\n",
    "    return x[None, ...]\n",
    "\n",
    "class ObjectDetector(object):\n",
    "    \n",
    "    def __init__(self, engine_path, preprocess_fn=bgr8_to_ssd_input):\n",
    "        logger = trt.Logger()\n",
    "        trt.init_libnvinfer_plugins(logger, '')\n",
    "        load_plugins()\n",
    "        self.trt_model = TRTModel(engine_path, input_names=[TRT_INPUT_NAME],output_names=[TRT_OUTPUT_NAME, TRT_OUTPUT_NAME + '_1'])\n",
    "        self.preprocess_fn = preprocess_fn\n",
    "        \n",
    "    def execute(self, *inputs):\n",
    "        trt_outputs = self.trt_model(self.preprocess_fn(*inputs))\n",
    "        return parse_boxes(trt_outputs)\n",
    "    \n",
    "    def __call__(self, *inputs):\n",
    "        return self.execute(*inputs)\n",
    "\n",
    "model = ObjectDetector('ssd_mobilenet_v2_coco.engine')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 Initialize the camera instance for the Intel realsense sensor D435i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use traitlets and widgets to display the image in Jupyter Notebook\n",
    "import traitlets\n",
    "from traitlets.config.configurable import SingletonConfigurable\n",
    "\n",
    "#use opencv to covert the depth image to RGB image for displaying purpose\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#using realsense to capture the color and depth image\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "#multi-threading is used to capture the image in real time performance\n",
    "import threading\n",
    "\n",
    "class Camera(SingletonConfigurable):\n",
    "    \n",
    "    # This changing of this value will be captured by traitlets\n",
    "    color_value = traitlets.Any()\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Configure the color and depth sensor\n",
    "        Set resolution for the color camera\n",
    "        Set resolution for the depth camera\n",
    "        Flag to control the thread\n",
    "        Start the RGBD sensor\n",
    "        Start capture the first color image\n",
    "        Start capture the first depth image\n",
    "        '''\n",
    "        super(Camera, self).__init__()\n",
    "        \n",
    "        #configure the color and depth sensor\n",
    "        self.pipeline = rs.pipeline()\n",
    "        self.configuration = rs.config()  \n",
    "        \n",
    "        #set resolution for the color camera\n",
    "        self.color_width = 640\n",
    "        self.color_height = 480\n",
    "        self.color_fps = 30\n",
    "        self.configuration.enable_stream(rs.stream.color, self.color_width, self.color_height, rs.format.bgr8, self.color_fps)\n",
    "\n",
    "        #set resolution for the depth camera\n",
    "        self.depth_width = 640\n",
    "        self.depth_height = 480\n",
    "        self.depth_fps = 30\n",
    "        self.configuration.enable_stream(rs.stream.depth, self.depth_width, self.depth_height, rs.format.z16, self.depth_fps)\n",
    "\n",
    "        #flag to control the thread\n",
    "        self.thread_runnning_flag = False\n",
    "        \n",
    "        #start the RGBD sensor\n",
    "        self.pipeline.start(self.configuration)\n",
    "        self.pipeline_started = True\n",
    "        frames = self.pipeline.wait_for_frames()\n",
    "\n",
    "        #start capture the first color image\n",
    "        color_frame = frames.get_color_frame()   \n",
    "        image = np.asanyarray(color_frame.get_data())\n",
    "        self.color_value = image\n",
    "\n",
    "        #start capture the first depth image\n",
    "        depth_frame = frames.get_depth_frame()           \n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "        self.depth_value = depth_colormap   \n",
    "\n",
    "    def _capture_frames(self):\n",
    "        '''This function is used for capturing the frames and processing them'''\n",
    "        while(self.thread_runnning_flag==True): #continue until the thread_runnning_flag is set to be False\n",
    "            frames = self.pipeline.wait_for_frames() #receive data from RGBD sensor\n",
    "            \n",
    "            color_frame = frames.get_color_frame() #get the color image\n",
    "            image = np.asanyarray(color_frame.get_data()) #convert color image to numpy array\n",
    "            self.color_value = image #assign the numpy array image to the color_value variable \n",
    "\n",
    "            depth_frame = frames.get_depth_frame() #get the depth image           \n",
    "            depth_image = np.asanyarray(depth_frame.get_data()) #convert depth data to numpy array\n",
    "            \n",
    "            \n",
    "            depth_image[:190,:]=0\n",
    "            depth_image[290:,:]=0\n",
    "            depth_image[:,:160]=0\n",
    "            depth_image[:,480:]=0\n",
    "            \n",
    "            #For object avoidance, we don't consider the distance that are lower than 100mm or bigger than 1000mm\n",
    "            depth_image[depth_image<100]=0\n",
    "            depth_image[depth_image>1000]=0\n",
    "            \n",
    "            #If all of the values in the depth image is 0, the depth[depth!=0] command will fail\n",
    "            #we set a specific value here to prevent this failure\n",
    "            depth_image[0,0]=2000\n",
    "            self.depth = depth_image\n",
    "            #conver depth data to BGR image for displaying purpose\n",
    "            depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "            self.depth_value = depth_colormap #assign the color BGR image to the depth value\n",
    "    \n",
    "    def start(self):\n",
    "        ''' Start the data capture thread'''\n",
    "        if self.thread_runnning_flag == False: #only process if no thread is running yet\n",
    "            self.thread_runnning_flag=True #flag to control the operation of the _capture_frames function\n",
    "            self.thread = threading.Thread(target=self._capture_frames) #link thread with the function\n",
    "            self.thread.start() #start the thread\n",
    "\n",
    "    def stop(self): \n",
    "        '''Stop the data capture thread'''\n",
    "        if self.thread_runnning_flag == True:\n",
    "            self.thread_runnning_flag = False #exit the while loop in the _capture_frames\n",
    "            self.thread.join() #wait the exiting of the thread       \n",
    "\n",
    "def bgr8_to_jpeg(value):\n",
    "    '''Convert numpy array to jpeg coded data for displaying '''\n",
    "    return bytes(cv2.imencode('.jpg',value)[1])\n",
    "\n",
    "#create a camera object\n",
    "camera = Camera.instance()\n",
    "camera.start() # start capturing the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Perform object detection and display the results on widgets\n",
    "Human is labeled is 1 in the pretrained model. A full list of the detection class indices can be found in the following link https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_complete_label_map.pbtxt\n",
    "\n",
    "The following program will ask the robot to follow if a human is detected in the captured color image. It will follow this human whilst maintaining a safe distance and avoiding collision with humans and objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Took Massive help from https://github.com/NVIDIA-AI-IOT/jetbot/blob/master/notebooks/object_following/live_demo.ipynb\n",
    "## Import Libraries \n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "import time\n",
    "from RobotClass import Robot\n",
    "# Initialize the Robot class\n",
    "robot = Robot()\n",
    "robot_speed = 0.4\n",
    "turning_speed = 2\n",
    "## set frame size and wiget size\n",
    "width = 640\n",
    "height = 480\n",
    "image_widget = widgets.Image(format='jpeg', width=300, height=300)\n",
    "label_widget = widgets.IntText(value=1, description='tracked label')\n",
    "stop_button = widgets.Button(description= 'Stop robot')\n",
    "   \n",
    "speed_widget = widgets.FloatSlider(value=0.4, min=0.0, max=1.0, description='speed')\n",
    "turn_gain_widget = widgets.FloatSlider(value=0.8, min=0.0, max=2.0, description='turn gain')\n",
    "\n",
    "def on_button_clicked():\n",
    "    '''Stop robot when user presses the button'''\n",
    "    robot.stop()\n",
    "\n",
    "stop_button.on_click(on_button_clicked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define all the functions responsible for the object detection. We are choosing the detected human with the largest bounding box. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_center(detection):\n",
    "        \"\"\"\n",
    "        Computes the center x, y coordinates of the object\n",
    "        Parameters: matching detection\n",
    "        Return: Center coordinates of the detection\n",
    "        \"\"\"\n",
    "        bbox = detection['bbox']\n",
    "        center_x = (bbox[0] + bbox[2]) / 2.0 - 0.5\n",
    "        center_y = (bbox[1] + bbox[3]) / 2.0 - 0.5\n",
    "        return (center_x, center_y)\n",
    "\n",
    "def detection_area(detection):\n",
    "        \"\"\"\n",
    "        Computes the area of the bbox of the object\"\n",
    "        Parameters: matching detection\n",
    "        Return: Area of the bounding box of the detection\n",
    "        \"\"\"\n",
    "        bbox = detection['bbox']\n",
    "        center_x = (bbox[2] - bbox[0]) \n",
    "        center_y = (bbox[3] - bbox[1])\n",
    "        return (center_x * center_y)\n",
    "    \n",
    "def closest_detection(detections):\n",
    "        \"\"\"\n",
    "        Finds the detection with the largest bbox area\"\"\n",
    "        Parameters: matching detection\n",
    "        Return: Detection with the largest bounding \n",
    "        \"\"\"    \n",
    "        closest_detection = None\n",
    "        for det in detections:\n",
    "                center = detection_center(det)\n",
    "                area= detection_area(det)\n",
    "                if closest_detection is None:\n",
    "                        closest_detection = det\n",
    "                elif (detection_area(det)) > (detection_area(closest_detection)):\n",
    "                        closest_detection = det\n",
    "        return closest_detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do the main processing. Here the robot follows the detected human with the largest bounding box. We have also implemented two speeds such that the robot increases its speed if the human is at a further distance. For collision avoidance the robot moves backwards if it detects an object with 200mm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f80feb25920d46f9ae2cb6d2cc510e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Image(value=b'', format='jpeg', height='300', width='300'),)), IntText(value=1, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Displays the camera stream\n",
    "display(widgets.VBox([\n",
    "        widgets.HBox([image_widget]),\n",
    "        label_widget,\n",
    "        speed_widget,\n",
    "        turn_gain_widget\n",
    "    ]))\n",
    "def processing(change):\n",
    "    '''This part is taking the data from all the fucntions above and making the decisions to move the robot in a particular direction'''\n",
    "    image = change['new']\n",
    "     \n",
    "    imgsized= cv2.resize(image,(300,300))\n",
    "    \n",
    "    # compute all detected objects\n",
    "    detections = model(imgsized)\n",
    "    \n",
    "    matching_detections = [d for d in detections[0] if d['label'] == int(label_widget.value)]\n",
    "    \n",
    "   # Create a blue bbox on the camera widget for each matching detection\n",
    "    for det in matching_detections:\n",
    "            bbox = det['bbox']\n",
    "            cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (255, 0, 0), 2)\n",
    "\n",
    "    # Create a green bbox for the matching detection with the largest detection\n",
    "    det = closest_detection(matching_detections)\n",
    "    if det is not None:\n",
    "        bbox = det['bbox']\n",
    "        cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (0, 255, 0), 5)\n",
    "    \n",
    "    # If no human is detected the robot will turn right until it finds one\n",
    "    if det is None:\n",
    "            robot.stop()\n",
    "            robot.right(0.5)\n",
    "            time.sleep(0.3)\n",
    "\n",
    "    # Otherwise move towards the detected human\n",
    "    else:\n",
    "        # Robot will move backwards if the human is too close to maintain a safe distance     \n",
    "        if (camera.depth[camera.depth!=0].min()<200):\n",
    "            robot.stop()\n",
    "            robot.backward(0.3)\n",
    "            time.sleep(0.3)\n",
    "        \n",
    "        # The robot stops at 700mm to maintain a safe distance and tracks human position.\n",
    "        elif (camera.depth[camera.depth!=0].min()<700):\n",
    "            center = detection_center(det)\n",
    "            robot.stop()\n",
    "            # If the human is on thr right side of the frame, the robot will turn right.            \n",
    "            if center[0]> 0.2:    \n",
    "                robot.right(0.4)\n",
    "                time.sleep(0.3)\n",
    "                robot.stop()\n",
    "            elif center[0]<-0.2: # If the human is on the left side of the frame, the robot will turn left.            \n",
    "                robot.left(0.4)\n",
    "                time.sleep(0.3)\n",
    "                robot.stop()        \n",
    "        \n",
    "        # Increase the speed if the distance from object is too far\n",
    "        elif (camera.depth[camera.depth!=0].min()>1200):\n",
    "            center = detection_center(det)\n",
    "            # Adjusts the motor values to move towards the center of the detected human\n",
    "            # In order to achivce maximum speed 0.6 has be added as 0.4 is the default value.\n",
    "            robot.set_motors(\n",
    "            float(speed_widget.value + 0.6 + turn_gain_widget.value * center[0]),\n",
    "            float(speed_widget.value + 0.6 - turn_gain_widget.value * center[0]),\n",
    "            float(speed_widget.value + 0.6 + turn_gain_widget.value * center[0]),\n",
    "            float(speed_widget.value + 0.6 - turn_gain_widget.value * center[0])\n",
    "            )\n",
    "\n",
    "        else:\n",
    "                # Go forward with normal speed.\n",
    "            center = detection_center(det)\n",
    "                \n",
    "                # Adjusts the motor values to move towards the center of the detected human\n",
    "            robot.set_motors(\n",
    "            float(speed_widget.value + turn_gain_widget.value * center[0]),\n",
    "            float(speed_widget.value - turn_gain_widget.value * center[0]),\n",
    "            float(speed_widget.value + turn_gain_widget.value * center[0]),\n",
    "            float(speed_widget.value - turn_gain_widget.value * center[0])\n",
    "            )\n",
    "\n",
    "\n",
    "    image_widget.value = bgr8_to_jpeg(image)\n",
    "    \n",
    "#the camera.observe function will monitor the color_value variable. If this value changes, the excecute function will be excuted.\n",
    "camera.observe(processing, names='color_value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now just Stop the camera and robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(500)\n",
    "camera.unobserve_all()\n",
    "camera.stop()\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "source[1]:https://learn.lboro.ac.uk/mod/resource/view.php?id=983989\n",
    "\n",
    "source[2]:https://github.com/NVIDIA-AI-IOT/jetbot/blob/master/notebooks/object_following/live_demo.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
